#### README ####
## Set working directory to the (unpacked) zipped folder that contains this code
## and associated data sets and PDFs. Do not put more files in this folder, as
## this will mess up the document mining later on.

## The code is written by several people, which is why different segments might
## have different coding styles (e.g. use of piping, length of each action etc.)

## The code used is largely based on that which has been presented in lectures
## and seminars, as well as the CRAN Documentation pages for each package.

## The reason why the mining code is placed last, even though the results are 
## relevant for part 2, is that a necessary package (tm) is masking functions 
## from other packages that we need when doing API requests. Therefore, the code 
## also may not act as expected if you run the code more than once without 
## resetting packages.

#### Packages ####
# Note: Some of these packages are already in the Tidyverse, but due to loading
# issues on some versions of R, we have included both Tidyverse and the
# necessary underlying packages separately.

library(tidyverse)
library(PxWebApiData)
library(rjstat)
library(sf)
library(rjson)
library(cowplot)
library(httr)
library(tibble)
library(dplyr)
library(ggplot2)
library(caret)
library(ranger)
library(gbm)
library(dynlm)
library(lmtest)
library(xts)
library(stargazer)
library(readxl)
library(magrittr)
library(lubridate)
library(rjstat)


#### Set working directory ####
setwd("FOLDER LOCATION")

#### Part 1: Map data ####
## creating map file ##

tmp1 <- tempfile()
download.file("http://api.thenmap.net/v2/no-4/geo/2008-01-01",
              destfile = tmp1
)
mapdata <- read_sf(tmp1)

## naming counties and correcting county numbers ##

tidymap<-arrange(mapdata, id)

tidymap$countynm <- c(
  "Østfold","Akershus","Oslo", "Hedmark", "Oppland", "Buskerud",
  "Vestfold", "Telemark", "Aust-Agder", "Vest-Agder", "Rogaland",
  "Hordaland", "Sogn og Fjordane", "Møre og Romsdal",
  "Sør-Trøndelag", "Nord-Trøndelag", "Nordland", "Troms",
  "Finnmark"
)

tidymap$id <-c(setdiff(101:120,113)) # Excluding 113 because it is old Bergen

View(tidymap)
plot(tidymap)


## Importing election results ##

# This is a standard API request from SSB, and will also be used later on for
# unemployment data

url <- "https://data.ssb.no/api/v0/no/table/08092/"

# spørring fra SSB

data <- '
{
  "query": [
    {
      "code": "Region",
      "selection": {
        "filter": "vs:ValgdistrikterMedBergen",
        "values": [
          "v01",
          "v02",
          "v03",
          "v04",
          "v05",
          "v06",
          "v07",
          "v08",
          "v09",
          "v10",
          "v11",
          "v12",
          "v14",
          "v15",
          "v16",
          "v17",
          "v18",
          "v19",
          "v20"
        ]
      }
    },
    {
      "code": "PolitParti",
      "selection": {
        "filter": "item",
        "values": [
          "02"
        ]
      }
    },
    {
      "code": "ContentsCode",
      "selection": {
        "filter": "item",
        "values": [
          "GodkjenteProsent"
        ]
      }
    },
    {
      "code": "Tid",
      "selection": {
        "filter": "item",
        "values": [
          "2009",
          "2013",
          "2017",
          "2021"
        ]
      }
    }
  ],
  "response": {
    "format": "json-stat2"
  }
}
'
d.tmp <- POST(url , body = data, encode = "json", verbose())

# Henter ut innholdet fra d.tmp som tekst deretter bearbeides av fromJSONstat

valgresultater <- fromJSONstat(content(d.tmp, "text"))

valgresultater[73:76,1]<-"Finnmark" # Easier name to work with

# Making a column for each election year

valgresultater<-pivot_wider(valgresultater,names_from=fireårlig,values_from=value) %>% 
  select(-statistikkvariabel,-"politisk parti") %>% 
  rename(countynm=region)

# Merging data frames ##

tidymap<-inner_join(valgresultater,tidymap, by =c("countynm"="countynm"))

## Plotting election results for 4 elections ##
p2021<- ggplot(tidymap$geometry)+
  geom_sf(aes(fill = tidymap$`2021`))+
  labs(title="FRP's result in the 2021 election",fill="result (pp)")+
  scale_fill_viridis_c(option="mako",direction=-1,limits=c(0,30)) +
  theme_void()

p2017 <- ggplot(tidymap$geometry)+
  geom_sf(aes(fill = tidymap$`2017`))+
  labs(title="FRP's result in the 2017 election",fill="result (pp)")+
  scale_fill_viridis_c(option="mako",direction=-1,limits=c(0,30)) +
  theme_void()

p2013 <- ggplot(tidymap$geometry)+
  geom_sf(aes(fill = tidymap$`2013`))+
  labs(title="FRP's result in the 2013 election",fill="result (pp)")+
  scale_fill_viridis_c(option="mako",direction=-1,limits=c(0,30)) +
  theme_void()

p2009 <- ggplot(tidymap$geometry)+
  geom_sf(aes(fill = tidymap$`2009`))+
  labs(title="FRP's result in the 2009 election",fill="result (pp)")+
  scale_fill_viridis_c(option="mako",direction=-1,limits=c(0,30)) +
  theme_void()

plot_grid(p2009,p2013,p2017,p2021)

#### Part 2: Regression and Machine Learning ####

rm(list = ls()) # Removing data from previous segment

set.seed(4170)

## Importing and editing datasets for keyword search ##

innvandring <- read_excel("innvandring.xlsx") %>%
  rename(innvandring = "1. januar 2008 - 30. september 2021") # Renaming for clarity

eldre <- read_excel("eldre.xlsx") %>%
  rename(eldre = "1. januar 2008 - 30. september 2021")

skattavgift <- read_excel("skattavgift.xlsx") %>%
  rename(skattavgift = "1. januar 2008 - 30. september 2021")

samferdsel <- read_excel("samferdsel.xlsx") %>%
  rename(samferdsel = "1. januar 2008 - 30. september 2021")

## Merging

artikler <- tibble(month = innvandring$Category,  # "Category" indicates month in original data frame
                   innvandring = innvandring$innvandring,
                   eldre = eldre$eldre,
                   skattavgift = skattavgift$skattavgift,
                   samferdsel=samferdsel$samferdsel) %>%
  mutate(tot.art = innvandring + eldre + skattavgift+samferdsel) %>% # Column for total number of articles
  mutate(month = format(as.Date(seq(as.Date("01-01-2008", "%d-%m-%Y"), # Changing "months" from text to date
                                    as.Date("30-09-2021", "%d-%m-%Y"),
                                    by = "month")), "%Y(%m)"))




## Importing unemployment data, same method as in Part 1 ##
url <- "https://data.ssb.no/api/v0/no/table/13332/"

data <- '
{
  "query": [
    {
      "code": "Kjonn",
      "selection": {
        "filter": "item",
        "values": [
          "0"
        ]
      }
    },
    {
      "code": "ContentsCode",
      "selection": {
        "filter": "item",
        "values": [
          "Arbeidslause4"
        ]
      }
    },
    {
      "code": "Tid",
      "selection": {
        "filter": "item",
        "values": [
          "2008M01",
          "2008M02",
          "2008M03",
          "2008M04",
          "2008M05",
          "2008M06",
          "2008M07",
          "2008M08",
          "2008M09",
          "2008M10",
          "2008M11",
          "2008M12",
          "2009M01",
          "2009M02",
          "2009M03",
          "2009M04",
          "2009M05",
          "2009M06",
          "2009M07",
          "2009M08",
          "2009M09",
          "2009M10",
          "2009M11",
          "2009M12",
          "2010M01",
          "2010M02",
          "2010M03",
          "2010M04",
          "2010M05",
          "2010M06",
          "2010M07",
          "2010M08",
          "2010M09",
          "2010M10",
          "2010M11",
          "2010M12",
          "2011M01",
          "2011M02",
          "2011M03",
          "2011M04",
          "2011M05",
          "2011M06",
          "2011M07",
          "2011M08",
          "2011M09",
          "2011M10",
          "2011M11",
          "2011M12",
          "2012M01",
          "2012M02",
          "2012M03",
          "2012M04",
          "2012M05",
          "2012M06",
          "2012M07",
          "2012M08",
          "2012M09",
          "2012M10",
          "2012M11",
          "2012M12",
          "2013M01",
          "2013M02",
          "2013M03",
          "2013M04",
          "2013M05",
          "2013M06",
          "2013M07",
          "2013M08",
          "2013M09",
          "2013M10",
          "2013M11",
          "2013M12",
          "2014M01",
          "2014M02",
          "2014M03",
          "2014M04",
          "2014M05",
          "2014M06",
          "2014M07",
          "2014M08",
          "2014M09",
          "2014M10",
          "2014M11",
          "2014M12",
          "2015M01",
          "2015M02",
          "2015M03",
          "2015M04",
          "2015M05",
          "2015M06",
          "2015M07",
          "2015M08",
          "2015M09",
          "2015M10",
          "2015M11",
          "2015M12",
          "2016M01",
          "2016M02",
          "2016M03",
          "2016M04",
          "2016M05",
          "2016M06",
          "2016M07",
          "2016M08",
          "2016M09",
          "2016M10",
          "2016M11",
          "2016M12",
          "2017M01",
          "2017M02",
          "2017M03",
          "2017M04",
          "2017M05",
          "2017M06",
          "2017M07",
          "2017M08",
          "2017M09",
          "2017M10",
          "2017M11",
          "2017M12",
          "2018M01",
          "2018M02",
          "2018M03",
          "2018M04",
          "2018M05",
          "2018M06",
          "2018M07",
          "2018M08",
          "2018M09",
          "2018M10",
          "2018M11",
          "2018M12",
          "2019M01",
          "2019M02",
          "2019M03",
          "2019M04",
          "2019M05",
          "2019M06",
          "2019M07",
          "2019M08",
          "2019M09",
          "2019M10",
          "2019M11",
          "2019M12",
          "2020M01",
          "2020M02",
          "2020M03",
          "2020M04",
          "2020M05",
          "2020M06",
          "2020M07",
          "2020M08",
          "2020M09",
          "2020M10",
          "2020M11",
          "2020M12",
          "2021M01",
          "2021M02",
          "2021M03",
          "2021M04",
          "2021M05",
          "2021M06",
          "2021M07",
          "2021M08"
        ]
      }
    }
  ],
  "response": {
    "format": "json-stat2"
  }
}
'

d.tmp <- POST(url , body = data, encode = "json", verbose())

unemployment <- fromJSONstat(content(d.tmp, "text")) %>% 
  mutate(month = format(as.Date(seq(as.Date("01-01-2008", "%d-%m-%Y"), # Changing months to match with other data frame
                                    as.Date("30-08-2021", "%d-%m-%Y"), 
                                    by = "month")), "%Y(%m)")) %>%
  rename(ue.rate=value) %>% 
  select(month,ue.rate) %>% # Removing irrelevant columns
  mutate(ue.change=ue.rate-lag(ue.rate)) # Creating column for change in unemployment rate (pp)

art.unemp <- inner_join(artikler,unemployment,by="month") # Merging articles and unemployment rate



## Importing, editing and merging poll stats ##

poll <- read.csv2("polls.csv") %>%
  mutate(pcts.frp = sub(" \\(.*", "", Frp)) %>% # Removing (representatives) from percentage column
  mutate(month = rev(format(as.Date(seq(as.Date("01-01-2008", "%d-%m-%Y"), # Changing months to match with other data frame
                                        as.Date("30-11-2021", "%d-%m-%Y"), 
                                        by = "month")), "%Y(%m)"))) %>%
  select(month, pcts.frp)

fullframe <- inner_join(art.unemp, poll, by = "month") %>% # Merging data frames where "month" is identical
  mutate(pcts.frp = as.numeric(sub(",", ".", pcts.frp))) # Changing from comma to period to identify numeric variables

rm(art.unemp,artikler,d.tmp,eldre,samferdsel,innvandring,poll,skattavgift,unemployment,data,url) # Removing the stuff we don't need

## Ordinary regression ##
# we use simple linear regression to compare different models of what could 
# affect FrPs results.
# It is possible that the number of articles or current unemployment rate does 
# not affect the polling results directly, so we gradually add lags to see if 
# they explain more.

fullframe.ts<-ts(fullframe,start=c(2008,1),end=c(2021,8),frequency=12) # Making it readable for time series models

mod1<-dynlm(pcts.frp~ue.rate, data=fullframe.ts)
summary(mod1)

mod2<-dynlm(pcts.frp~ue.rate + eldre+innvandring+skattavgift+samferdsel, data=fullframe.ts)
summary(mod2)

mod3<-dynlm(pcts.frp~ue.rate + L(eldre,0:1)+L(innvandring,0:1)+L(skattavgift,0:1)+L(samferdsel,0:1), data=fullframe.ts)
summary(mod3)

mod4<-dynlm(pcts.frp~L(ue.rate,0:1) + L(eldre,0:1)+L(innvandring,0:1)+L(skattavgift,0:1), data=fullframe.ts)
summary(mod4) # See that R^2 decreases even though number of regressors go up and new regressors are insignificant, so we "go back" to mod 3

# does total number of articles matter more than individual ?
mod5<-dynlm(pcts.frp~ue.rate+L(tot.art,0:1),data=fullframe.ts)
summary(mod5)
#no (see SSE and R^2). Continue with mod3

# Exporting regressions to table with stargazer

stargazer(mod1,mod2,mod3,mod4,mod5,
          type = "html",
          out = "regtable-frp.doc"
)

rm(mod1,mod2,mod4,mod5)

## Machine learning ##

# Create learning frame, removing irrelevant and double-counting columns

learning.frame <- fullframe %>% 
  select(-month,-tot.art,-ue.change) %>%
  mutate(lag.eldre=lag(eldre),lag.innvandring=lag(innvandring),lag.skattavgift=lag(skattavgift),lag.samferdsel=lag(samferdsel)) %>% 
  na.omit()

# Because we want to predict the result in 2021, we let the machine train on 
# data from 2008-2016 and test it on 2017-2021.

frp.train <- learning.frame[1:107,] # Dataset for training, 2008(2)-2016(12)
frp.test <- learning.frame[108:nrow(learning.frame), ] # Dataset for testing, 2017(1)-2021(8)

# We train the model with out-of-sample testing. To do so, we split the data set 
# in 10 and then in 10 again, taking 9/10 in each group as training and the last as test.

ctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 10)

## Method 1: OLS

ols.mod <- train(pcts.frp ~ .,
                 data = frp.train,
                 method = "lm",
                 trControl = ctrl
)

ols.mod

## Method 2: Random forest
rf.mod <- train(pcts.frp ~ .,
                data = frp.train,
                method = "ranger", # code for random forest method given in problem text
                importance = "permutation", # Argument used to obtain varImp in next problem (given in problem text)
                trControl = ctrl
)

rf.mod

## Method 3: Boosting Tree

bt.mod <- train(pcts.frp ~ .,
                data = frp.train,
                method = "gbm",
                trControl = ctrl
)
bt.mod

varImp(ols.mod)
varImp(rf.mod)
varImp(bt.mod) 

## Prediction/Testing the models ##
ols.pred <- predict(ols.mod, frp.test)
rf.pred <- predict(rf.mod, frp.test)
bt.pred <- predict(bt.mod, frp.test)

# Creating results matrix and estimating Mean Squared Errors (MSE)

results <- tibble(ols.pred, rf.pred, bt.pred, 
                  pcts.frp = frp.test$pcts.frp) %>% 
  mutate(ols.error=ols.pred-pcts.frp, #columns for errors
         rf.error=rf.pred-pcts.frp,
         bt.error=bt.pred-pcts.frp) %>% 
  mutate(mse.ols=mean(ols.error^2), #mean square errors
         mse.rf=mean(rf.error^2),
         mse.bt=mean(bt.error^2))

# Boosting Tree has lowest mean square errors

summary(bt.mod)

## Check histogram of errors

results %>%
  ggplot() +
  geom_density(aes(x = ols.error, color = "ols error")) +
  geom_density(aes(x = rf.error, color = "random forest error")) +
  geom_density(aes(x = bt.error, color = "boosting tree error"))+
  labs(y="Density",x="Error")+
  ggtitle("Density plot of errors")+
  geom_vline(xintercept=0)

# "Agree" that Boosting Tree is most accurate

## Plot predictions and true results together

results<- results %>% 
  mutate(testmonth=c(1:56)) # This is just to have a functioning x-axis

results %>% 
  ggplot()+
  geom_line(aes(x=testmonth,y=pcts.frp, color="Actual result"))+
  geom_line(aes(x=testmonth,y=ols.pred,color="OLS prediction"))+
  geom_line(aes(x=testmonth,y=rf.pred,color="Random forest prediction"))+
  geom_line(aes(x=testmonth,y=bt.pred,color="Boosting tree prediction"))+
  labs(y="Percentage points",x="Month number in test period")+
  ggtitle("Actual vs. predicted results")

print(results[56,]) ## see predicted values of last available month - august 2021


#### Part 3: Text mining ####
## Necessary packages for part 3 ##
library(tm)
library(pdftools)
library(writexl)

rm(list=ls()) #removing data from previous segment

## Importing PDFs ##

FRP_PDFs <- list.files(pattern = "pdf$")


# Creating list
party_programs <- lapply(FRP_PDFs, pdf_text)


# Checking list length to confirm the four pdfs have been imported
length(party_programs)


# Checking length of each pdf to confirm all pages are included
# The party programs for 2009-13; 2013-17; 2017-21; 2021-25 should 
# have 92, 86, 90 and 132 pages, respectively.
lapply(party_programs, length)


# Creating the Party Program vector
PP_vector <- Corpus(VectorSource(party_programs))

# Inspect the program for 2009-13
inspect(PP_vector[1:1])

## Cleaning the corpus ##

# Creating cleaning function
cleanityMcClean <- function(PP_vector){
  PP_vector <- tm_map(PP_vector, content_transformer(tolower))
  PP_vector <- tm_map(PP_vector, removePunctuation)
  PP_vector <- tm_map(PP_vector, removeNumbers)
  PP_vector <- tm_map(PP_vector, removeWords, words= stopwords("norwegian"))
  PP_vector <- tm_map(PP_vector, stripWhitespace)
  PP_vector <- tm_map(PP_vector, content_transformer(gsub), pattern="\\W",replace=" ")
  return(PP_vector)
}

PP_vector1 <- cleanityMcClean(PP_vector)

# create term matrix PP_dtm
PP_dtm <- TermDocumentMatrix(PP_vector1, 
                             control = list(bounds = list(global = c(3,Inf))))

FT <-  findFreqTerms(PP_dtm, lowfreq = 40,highfreq = Inf) # Using 40 as lowest frequency of words

# Creating matrix of alphabetized words and count in  each document
FT_dtm <- as.matrix(PP_dtm[FT,])

# Sorting after total frequency
Frequent_Terms <- sort(apply(FT_dtm,1,sum), decreasing = TRUE)
Frequent_Terms

# Extracting table to Excel

df_Frequent_Terms <- data.frame(Frequent_Terms) 
df_Frequent_Terms$words <- rownames(df_Frequent_Terms)
write_xlsx(df_Frequent_Terms,"Frequent_Terms_FrP.xlsx")
