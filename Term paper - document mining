library(tm)
library(pdftools)

## IMPORTING PDFs ##

# Set working directory to the folder containing the party program pdfs
setwd("C:/Users/linns/OneDrive/Dokumenter/Skole/4170 - Data Science/4170 R/Partiprogrammer")
FRP_PDFs <- list.files(pattern = "pdf$")


# Creating list
party_programs <- lapply(FRP_PDFs, pdf_text)


# Checking list length to confirm the four pdfs have been imported
length(party_programs)


# Checking length of each pdf to confirm all pages are included
# The party programs for 2009-13; 2013-17; 2017-21; 2021-25 should 
# have 92, 86, 90 and 132 pages, respectively.
lapply(party_programs, length)


# Creating the Party Program vector
PP_vector <- Corpus(VectorSource(party_programs))

# Inspect the program for 2009-13
inspect(PP_vector[1:1])

## Cleaning the corpus ##

# Creating cleaning function
cleanityMcClean <- function(PP_vector){
  PP_vector <- tm_map(PP_vector, content_transformer(tolower))
  PP_vector <- tm_map(PP_vector, removePunctuation)
  PP_vector <- tm_map(PP_vector, removeNumbers)
  PP_vector <- tm_map(PP_vector, removeWords, words= stopwords("norwegian"))
  PP_vector <- tm_map(PP_vector, stripWhitespace)
  return(PP_vector)
}

PP_vector1 <- cleanityMcClean(PP_vector)

# create term matrix PP_dtm
PP_dtm <- TermDocumentMatrix(PP_vector1, 
                             control = list(bounds = list(global = c(3,Inf))))

FT <-  findFreqTerms(PP_dtm, lowfreq = 40,highfreq = 1000)
as.matrix(PP_dtm[FT,])

#Sort the count of all frequently
FT_dtm <- as.matrix(PP_dtm[FT,])

sort(apply(FT_dtm,1,sum), decreasing = TRUE)
     
